{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split,  GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from umap import UMAP\n",
    "import preprocess\n",
    "import numpy as np \n",
    "\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.ensemble import AdaBoostClassifier, StackingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and preprocess data\n",
    "df = pd.read_excel('./clean_dataset/data-base.xlsx')\n",
    "df = df.dropna()\n",
    "df.drop(columns=['link'], inplace=True)\n",
    "df = preprocess.preprocess(df)\n",
    "\n",
    "df['title'] = df['title'].astype('str')\n",
    "# Split data into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['severity_rating'], random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare data, extract embeddings, and prepare sub-models\n",
    "docs = train_df['title']\n",
    "docs = docs.reset_index(drop=True)\n",
    "\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings_train = sentence_model.encode(docs, show_progress_bar=True)\n",
    "\n",
    "# We reduce our embeddings to 2D as it will allows us to quickly iterate later on\n",
    "reduced_embeddings = UMAP(n_neighbors=10, n_components=2, \n",
    "                          min_dist=0.0, metric='cosine').fit_transform(embeddings_train)\n",
    "\n",
    "# Train our topic model\n",
    "topic_model = BERTopic(embedding_model=sentence_model, umap_model=umap_model, \n",
    "                       vectorizer_model=vectorizer_model, calculate_probabilities=True, nr_topics=40)\n",
    "topics_train, probs = topic_model.fit_transform(docs, embeddings_train)\n",
    "\n",
    "\n",
    "new_topics = topic_model.reduce_outliers(docs, topics_train, probabilities=probs, \n",
    "                             threshold=0.05, strategy=\"embeddings\")\n",
    "topic_model.update_topics(docs, topics=new_topics)\n",
    "\n",
    "# Add topics to the dataframe\n",
    "train_df['topics'] = new_topics\n",
    "\n",
    "# Transform test data using the trained topic model (without retraining)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "topics_test, probs = topic_model.transform(test_df['title'])\n",
    "\n",
    "# Map outliers to the closest topic based on topic probabilities\n",
    "outlier_indices = [i for i, topic in enumerate(topics_test) if topic == -1]\n",
    "for idx in outlier_indices:\n",
    "    topics_test[idx] = np.argmax(probs[idx])\n",
    "\n",
    "test_df['topics'] = topics_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map topics to their representative words for both training and test data\n",
    "topic_info = topic_model.get_topic_info()\n",
    "train_df['topic_words'] = train_df['topics'].apply(lambda x: topic_info.loc[x, 'Representation'] if x != -1 else \"NoisyTopic\")\n",
    "test_df['topic_words'] = test_df['topics'].apply(lambda x: topic_info.loc[x, 'Representation'] if x != -1 else \"NoisyTopic\")\n",
    "\n",
    "train_df['topic_words_str'] = train_df['topic_words'].apply(' '.join)\n",
    "test_df['topic_words_str'] = test_df['topic_words'].apply(' '.join)\n",
    "\n",
    "# Generate features for training and test data\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_words = vectorizer.fit_transform(train_df['topic_words_str'])\n",
    "X_test_words = vectorizer.transform(test_df['topic_words_str'])\n",
    "\n",
    "# Map topics to predominant severity using only the training data\n",
    "topic_severity_mapping = {}\n",
    "for topic in train_df['topics'].unique():\n",
    "    predominant_severity = train_df[train_df['topics'] == topic]['severity_rating'].mode()[0]\n",
    "    topic_severity_mapping[topic] = predominant_severity\n",
    "\n",
    "# Apply the mapping to both training and test data\n",
    "train_df['topic_severity'] = train_df['topics'].map(topic_severity_mapping)\n",
    "test_df['topic_severity'] = test_df['topics'].map(lambda x: topic_severity_mapping.get(x, \"Unknown\"))\n",
    "\n",
    "le2 = LabelEncoder()\n",
    "train_df['encoded_topic_severity'] = le2.fit_transform(train_df['topic_severity'])\n",
    "test_df['encoded_topic_severity'] = le2.transform(test_df['topic_severity'])\n",
    "# One-hot encode the 'vuln_type' column for both training and test data\n",
    "train_vuln_type_encoded = pd.get_dummies(train_df['vuln_type'], prefix='vuln_type')\n",
    "test_vuln_type_encoded = pd.get_dummies(test_df['vuln_type'], prefix='vuln_type')\n",
    "\n",
    "# Ensure that both training and test data have the same columns after one-hot encoding\n",
    "missing_cols = set(train_vuln_type_encoded.columns) - set(test_vuln_type_encoded.columns)\n",
    "for c in missing_cols:\n",
    "    test_vuln_type_encoded[c] = 0\n",
    "test_vuln_type_encoded = test_vuln_type_encoded[train_vuln_type_encoded.columns]\n",
    "\n",
    "# Combine with the original features\n",
    "X_train_combined = pd.concat([train_df[['encoded_topic_severity']].reset_index(drop=True), \n",
    "                              pd.DataFrame(X_train_words.toarray(), columns=vectorizer.get_feature_names_out()),\n",
    "                              train_vuln_type_encoded.reset_index(drop=True)], axis=1)\n",
    "X_test_combined = pd.concat([test_df[['encoded_topic_severity']].reset_index(drop=True), \n",
    "                             pd.DataFrame(X_test_words.toarray(), columns=vectorizer.get_feature_names_out()),\n",
    "                             test_vuln_type_encoded.reset_index(drop=True)], axis=1)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(train_df['severity_rating'])\n",
    "y_test = le.transform(test_df['severity_rating'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#####--->>>>>>IMBALANCE<<--------####################\n",
    "X_train_resampled, y_train_resampled = X_train_combined, y_train\n",
    "\n",
    "\n",
    "\n",
    "####------>>SMOTEEN<<------------- #########\n",
    "\n",
    "#sme = SMOTEENN(smote=smote, random_state=42)\n",
    "#X_train_resampled, y_train_resampled = sme.fit_resample(X_train_combined, y_train)\n",
    "\n",
    "\n",
    "######--------->>>>RUS<<<<<--------------#####\n",
    "#rus = RandomUnderSampler(random_state=42)\n",
    "#X_train_resampled, y_train_resampled = rus.fit_resample(X_train_combined, y_train)\n",
    "\n",
    "\n",
    "####------->>>>>>>SMOTE<<<<-----------------#######\n",
    "#smote = SMOTE(random_state=42)\n",
    "#X_train_resampled, y_train_resampled = smote.fit_resample(X_train_combined, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#########--->>>>>>>STACKING<<--------############\n",
    "\n",
    "# Use Decision Tree as the base estimator for AdaBoost\n",
    "base_estimator = DecisionTreeClassifier(max_depth=1)\n",
    "boosted_tree = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=50, random_state=42)\n",
    "\n",
    "# Define the Naive Bayes model\n",
    "naive_bayes = GaussianNB()\n",
    "\n",
    "# Define the stacking classifier\n",
    "estimators = [\n",
    "    ('boosted_tree', boosted_tree),\n",
    "    ('naive_bayes', naive_bayes)\n",
    "]\n",
    "stacking_classifier = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
    "\n",
    "# Define hyperparameters to tune\n",
    "param_grid = {\n",
    "    'boosted_tree__n_estimators': [30, 50, 70],\n",
    "    'boosted_tree__base_estimator__max_depth': [1, 2, 3],\n",
    "    'final_estimator__C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# Use GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(stacking_classifier, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)  \n",
    "\n",
    "# Train the stacking classifier with best parameters\n",
    "best_stacking_classifier = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "\n",
    "x_pred = best_stacking_classifier.predict(X_train_resampled)\n",
    "train_accuracy = np.mean(x_pred == y_train_resampled)\n",
    "print(f\"train Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the stacked model with best parameters\n",
    "y_pred = best_stacking_classifier.predict(X_test_combined)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "y_pred = best_stacking_classifier.predict(X_test_combined)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a dictionary mapping the original classes to their encoded values\n",
    "class_mapping = {label: idx for idx, label in enumerate(le.classes_)}\n",
    "print(class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####----training matrix------########\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "x_pred = best_stacking_classifier.predict(X_train_resampled)\n",
    "print(classification_report(y_train_resampled, x_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\")\n",
    "print(confusion_matrix(y_train_resampled, x_pred))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
